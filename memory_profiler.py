#!/usr/bin/env python3
"""
å·¥ä¸šçº§ AI è®°å¿†åˆ†æå™¨ (Industrial-Scale AI Memory Profiler)

åŸºäº Map-Reduce æ¶æ„ï¼Œä» 1500+ å¼ ç…§ç‰‡ä¸­é‡æ„ç”¨æˆ·çš„ã€Šå…­ç»´äººæ ¼ç”»åƒã€‹

Phase 1 (Map): Gemini Flash å¿«é€Ÿæå–æ¯æ‰¹ç…§ç‰‡çš„å®¢è§‚äº‹å®
Phase 2 (Reduce): Gemini Pro æ·±åº¦åˆ†æç”Ÿæˆå®Œæ•´äººæ ¼ç”»åƒ

ç¼“å­˜æœºåˆ¶ï¼šPhase 1 ç»“æœæŒä¹…åŒ–ï¼Œæ”¯æŒæ— é™æ¬¡è°ƒæ•´ Phase 2 åˆ†æé€»è¾‘
"""

import asyncio
import json
import os
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass, asdict
from PIL import Image
import google.genai as genai
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor
import logging

# ==================== é…ç½® ====================
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('memory_profiler.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Gemini API é…ç½®
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))

# ==================== æ•°æ®æ¨¡å‹ ====================

@dataclass
class PhotoMetadata:
    """ç…§ç‰‡å…ƒæ•°æ®"""
    path: str
    filename: str
    datetime: datetime
    gps_lat: float = None
    gps_lon: float = None
    has_gps: bool = False

@dataclass
class Batch:
    """æ‰¹æ¬¡ä¿¡æ¯"""
    batch_id: str  # æ ¼å¼: YYYY-MM_Wxx
    photos: List[PhotoMetadata]
    time_range: Tuple[datetime, datetime]
    image_count: int

@dataclass
class Phase1Result:
    """Phase 1 åˆ†æç»“æœï¼ˆå®¢è§‚äº‹å®ï¼‰"""
    batch_id: str
    processed_at: str
    image_count: int
    time_range: Tuple[str, str]
    raw_vlm_output: str  # Gemini Flash çš„å…¨é‡å®¢è§‚æè¿°

# ==================== Phase 0: EXIF æå–å™¨ ====================

class EXIFExtractor:
    """ä»ç…§ç‰‡ä¸­æå– EXIF å…ƒæ•°æ®"""

    @staticmethod
    def get_decimal_from_dms(dms, ref):
        """å°† GPS DMS æ ¼å¼è½¬ä¸º Decimal"""
        degrees = dms[0]
        minutes = dms[1]
        seconds = dms[2]

        decimal = degrees + (minutes / 60.0) + (seconds / 3600.0)
        if ref in ['S', 'W']:
            decimal = -decimal
        return decimal

    @staticmethod
    def extract_datetime(image_path: str) -> datetime:
        """æå–æ‹æ‘„æ—¶é—´"""
        try:
            with Image.open(image_path) as img:
                exif = img._getexif()
                if exif:
                    # DateTimeOriginal (36867)
                    date_str = exif.get(36867) or exif.get(306)  # DateTime (306)
                    if date_str:
                        # æ ¼å¼: "2023:12:25 14:30:00"
                        return datetime.strptime(date_str, "%Y:%m:%d %H:%M:%S")
        except Exception as e:
            logger.warning(f"æ— æ³•æå–æ—¶é—´æˆ³ {image_path}: {e}")

        # å¤‡ç”¨ï¼šä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´
        return datetime.fromtimestamp(os.path.getmtime(image_path))

    @staticmethod
    def extract_gps(image_path: str) -> Tuple[float, float]:
        """æå– GPS åæ ‡"""
        try:
            with Image.open(image_path) as img:
                exif = img._getexif()
                if exif and 34853 in exif:  # GPSInfo
                    gps_info = exif[34853]

                    lat = None
                    lon = None

                    # çº¬åº¦
                    if 2 in gps_info and 1 in gps_info:  # GPSLatitude, GPSLatitudeRef
                        lat = EXIFExtractor.get_decimal_from_dms(
                            gps_info[2], gps_info[1]
                        )

                    # ç»åº¦
                    if 4 in gps_info and 3 in gps_info:  # GPSLongitude, GPSLongitudeRef
                        lon = EXIFExtractor.get_decimal_from_dms(
                            gps_info[4], gps_info[3]
                        )

                    return lat, lon
        except Exception as e:
            logger.warning(f"æ— æ³•æå–GPS {image_path}: {e}")

        return None, None

    @classmethod
    def process_photo(cls, photo_path: str) -> PhotoMetadata:
        """å¤„ç†å•å¼ ç…§ç‰‡ï¼Œæå–æ‰€æœ‰å…ƒæ•°æ®"""
        filename = os.path.basename(photo_path)
        dt = cls.extract_datetime(photo_path)
        lat, lon = cls.extract_gps(photo_path)

        return PhotoMetadata(
            path=photo_path,
            filename=filename,
            datetime=dt,
            gps_lat=lat,
            gps_lon=lon,
            has_gps=(lat is not None and lon is not None)
        )

# ==================== Phase 0: æ™ºèƒ½æ‰¹æ¬¡åˆ†ç»„å™¨ ====================

class BatchProcessor:
    """æŒ‰æ—¶é—´æ™ºèƒ½åˆ†ç»„ç…§ç‰‡"""

    @staticmethod
    def create_batches(photos: List[PhotoMetadata], max_batch_size: int = 50) -> List[Batch]:
        """
        æ™ºèƒ½åˆ†ç»„ç­–ç•¥ï¼š
        1. ä¼˜å…ˆæŒ‰æœˆåˆ†ç»„
        2. å¦‚æœæŸæœˆ > max_batch_sizeï¼ŒæŒ‰å‘¨æ‹†åˆ†
        """
        # æŒ‰æœˆåˆ†ç»„
        monthly_groups = {}
        for photo in photos:
            month_key = photo.datetime.strftime("%Y-%m")
            if month_key not in monthly_groups:
                monthly_groups[month_key] = []
            monthly_groups[month_key].append(photo)

        batches = []
        for month_key, month_photos in sorted(monthly_groups.items()):
            if len(month_photos) <= max_batch_size:
                # æ•´æœˆä½œä¸ºä¸€ä¸ªæ‰¹æ¬¡
                batch_id = month_key
                time_range = (
                    min(p.datetime for p in month_photos),
                    max(p.datetime for p in month_photos)
                )
                batches.append(Batch(
                    batch_id=batch_id,
                    photos=sorted(month_photos, key=lambda x: x.datetime),
                    time_range=time_range,
                    image_count=len(month_photos)
                ))
            else:
                # æŒ‰å‘¨æ‹†åˆ†
                week_groups = {}
                for photo in month_photos:
                    # è®¡ç®—å‘¨æ•°ï¼ˆä¸€å¹´ä¸­çš„ç¬¬å‡ å‘¨ï¼‰
                    week_num = photo.datetime.isocalendar()[1]
                    week_key = f"{month_key}_W{week_num:02d}"
                    if week_key not in week_groups:
                        week_groups[week_key] = []
                    week_groups[week_key].append(photo)

                for week_key, week_photos in sorted(week_groups.items()):
                    time_range = (
                        min(p.datetime for p in week_photos),
                        max(p.datetime for p in week_photos)
                    )
                    batches.append(Batch(
                        batch_id=week_key,
                        photos=sorted(week_photos, key=lambda x: x.datetime),
                        time_range=time_range,
                        image_count=len(week_photos)
                    ))

        return batches

# ==================== Phase 1: Map - å®¢è§‚äº‹å®æå– ====================

class Phase1Analyzer:
    """ä½¿ç”¨ Gemini Flash å¿«é€Ÿæå–å®¢è§‚äº‹å®"""

    def __init__(self):
        # ä½¿ç”¨ Gemini 2.5 Flash ç¡®ä¿æ›´å‡†ç¡®çš„è§†è§‰è¯†åˆ«
        self.model = genai.GenerativeModel('models/gemini-2.5-flash')
        self.cache_dir = Path("cache/phase1")
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # å°è¯•åŠ è½½ä¸»è§’ç‰¹å¾
        self.protagonist_features = self._load_protagonist_features()

    def _load_protagonist_features(self) -> dict:
        """åŠ è½½ä¸»è§’ç‰¹å¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
        protagonist_path = Path("cache/protagonist_features.json")
        if protagonist_path.exists():
            try:
                with open(protagonist_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    logger.info(f"âœ… å·²åŠ è½½ä¸»è§’å‚è€ƒç‰¹å¾: {data['features']['key_identifiers']}")
                    return data['features']
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½ä¸»è§’ç‰¹å¾: {e}")
        return None

    def _get_cache_path(self, batch_id: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"batch_{batch_id}.json"

    def _load_from_cache(self, batch_id: str) -> Phase1Result:
        """ä»ç¼“å­˜åŠ è½½ç»“æœ"""
        cache_path = self._get_cache_path(batch_id)
        if cache_path.exists():
            logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½æ‰¹æ¬¡ {batch_id}")
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return Phase1Result(**data)
        return None

    def _save_to_cache(self, result: Phase1Result):
        """ä¿å­˜ç»“æœåˆ°ç¼“å­˜"""
        cache_path = self._get_cache_path(result.batch_id)
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(result), f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ å·²ç¼“å­˜æ‰¹æ¬¡ {result.batch_id}")

    def _create_prompt(self, batch: Batch) -> str:
        """ç”Ÿæˆ Phase 1 æç¤ºè¯ï¼ˆå®¢è§‚äº‹å®æå–ï¼‰"""
        # æå–æ—¶é—´èŒƒå›´å’Œ GPS ä¿¡æ¯
        time_str = f"{batch.time_range[0].strftime('%Yå¹´%mæœˆ')} - {batch.time_range[1].strftime('%Yå¹´%mæœˆ')}"

        # ç»Ÿè®¡æœ‰ GPS çš„ç…§ç‰‡æ•°é‡
        gps_count = sum(1 for p in batch.photos if p.has_gps)
        location_info = f"å…¶ä¸­ {gps_count} å¼ åŒ…å«GPSå®šä½ä¿¡æ¯" if gps_count > 0 else "æ— GPSä¿¡æ¯"

        # æ„å»ºä¸»è§’ç‰¹å¾æç¤º
        protagonist_info = ""
        if self.protagonist_features:
            features = self.protagonist_features
            protagonist_info = f"""

**ğŸ¯ ä¸»è§’å‚è€ƒç‰¹å¾**ï¼ˆåŸºäºç”¨æˆ·æä¾›çš„å‚è€ƒç…§ç‰‡ï¼‰:
- æ€§åˆ«: {features['gender']}
- å¹´é¾„æ®µ: {features['age_group']}
- å‘å‹: {features['facial_features']['hair']}
- è„¸å‹: {features['facial_features']['face_shape']}
- å…³é”®è¯†åˆ«ç‰¹å¾:
"""
            for identifier in features['key_identifiers']:
                protagonist_info += f"  * {identifier}\n"

        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è§†è§‰äººç±»å­¦å®¶ã€‚è¯·è¯¦ç»†æè¿°è¿™æ‰¹ç…§ç‰‡ä¸­çš„æ‰€æœ‰è§†è§‰è¦ç´ ã€‚

**æ‰¹æ¬¡ä¿¡æ¯**:
- æ—¶é—´èŒƒå›´: {time_str}
- ç…§ç‰‡æ•°é‡: {batch.image_count} å¼ 
- {location_info}
{protagonist_info}
**âš ï¸ æœ€é‡è¦ä»»åŠ¡: è¯†åˆ«ç…§ç‰‡ä¸»äººï¼ˆä¸»è§’ï¼‰**

0. **ä¸»è§’è¯†åˆ«** (ä¼˜å…ˆçº§æœ€é«˜):{"ã€ä¼˜å…ˆã€‘è¯·æ ¹æ®ä¸Šé¢çš„ã€ä¸»è§’å‚è€ƒç‰¹å¾ã€‘å‡†ç¡®è¯†åˆ«ç…§ç‰‡ä¸­çš„ä¸»è§’ã€‚" if self.protagonist_features else ""}
   - **è°æ˜¯ä¸»äºº**: é€šè¿‡ä»¥ä¸‹çº¿ç´¢ç»¼åˆåˆ¤æ–­ï¼š
     * å‡ºç°é¢‘ç‡æœ€é«˜çš„äººç‰©
     * ç…§ç‰‡è§†è§’ï¼ˆè‡ªæ‹ = ä¸»äººï¼Œè¢«æ‹ = ä¸»äººä¸åœ¨ç”»é¢ä¸­ï¼‰
     * é•œå¤´è·ç¦»ï¼ˆé€šå¸¸æ˜¯ä¸»ä½“äººç‰©ï¼Œä½äºç”»é¢ä¸­å¿ƒæˆ–å‰æ™¯ï¼‰
     * äº’åŠ¨æ¨¡å¼ï¼ˆé€šå¸¸æ˜¯è¢«æ‹æ‘„çš„å¯¹è±¡ï¼Œæˆ–åœ¨åˆå½±ä¸­ä½äºä¸­å¿ƒï¼‰
   - **ä¸»è§’ç‰¹å¾**: è¯¦ç»†æè¿°ä¸»è§’çš„å¤–è²Œç‰¹å¾ï¼ˆæ€§åˆ«ã€å¹´é¾„æ®µã€å‘å‹ã€ä½“å‹ã€å¸¸è§æœè£…é£æ ¼ï¼‰
   - **æ ‡è®°æ–¹å¼**: åœ¨åç»­æè¿°ä¸­ï¼Œä¸»è§’ç”¨"ã€ä¸»è§’ã€‘"æ ‡è®°ï¼Œå…¶ä»–äººç”¨"å¥³æ€§A"ã€"ç”·æ€§B"ç­‰æ ‡è®°
   - **ç‰¹æ®Šåœºæ™¯**:
     * å¦‚æœæ˜¯è‡ªæ‹ï¼Œç›´æ¥æ ‡è®°"ã€ä¸»è§’ã€‘è‡ªæ‹"
     * å¦‚æœæ˜¯å¤šäººå¤§åˆå½±ï¼Œä¸»äººé€šå¸¸åœ¨ä¸­å¿ƒä½ç½®æˆ–æœ€æ˜¾çœ¼çš„ä½ç½®
     * å¦‚æœä¸»äººä¸åœ¨ç…§ç‰‡ä¸­ï¼ˆå¦‚ä¸»äººæ‹çš„é£æ™¯ç…§ï¼‰ï¼Œè¯´æ˜"ã€ä¸»äººã€‘æ‹æ‘„ï¼Œæœªå…¥é•œ"

**ä»»åŠ¡**: è¯·ä»¥è¯¦å°½ã€å®¢è§‚çš„æ–¹å¼æè¿°è¿™æ‰¹ç…§ç‰‡ï¼ŒåŒ…å«ä»¥ä¸‹ç»´åº¦ï¼š

1. **åœºæ™¯ä¸ç¯å¢ƒ** (å®Œæ•´è¯†åˆ«):
   - å®¤å†…/å®¤å¤–åœºæ™¯ï¼ˆå¦‚ï¼šå’–å•¡å…ã€åŠå…¬å®¤ã€å…¬å›­ã€å®¶ä¸­ã€é¤å…ï¼‰
   - åŸå¸‚/åœ°ç‚¹çº¿ç´¢ï¼ˆå¦‚ï¼šä¸Šæµ·å¤–æ»©ã€ä¸œäº¬è¡—å¤´ã€å±…å®¶ç¯å¢ƒã€å•†åœºï¼‰
   - è£…ä¿®é£æ ¼ã€ç©ºé—´æ°›å›´ï¼ˆå¦‚ï¼šç°ä»£ç®€çº¦ã€å¤å¤å·¥ä¸šã€æ¸©é¦¨å±…å®¶ï¼‰

2. **äººç‰©ä¸äº’åŠ¨** (å®Œæ•´è¯†åˆ«ï¼Œé‡ç‚¹æ ‡æ³¨ä¸»è§’):
   - **ä¸»è§’**: ã€ä¸»è§’ã€‘çš„æ€§åˆ«ã€å¹´é¾„æ®µã€å¤–è²Œç‰¹å¾ï¼ˆå‘å‹ã€èº«æã€å¸¸è§æœè£…é£æ ¼ï¼‰
   - **å…¶ä»–äººç‰©**: æ•°é‡ã€æ€§åˆ«ã€å¹´é¾„æ®µï¼Œç”¨"å¥³æ€§A"ã€"ç”·æ€§B"æ ‡è®°
   - **äººç‰©å…³ç³»**: ã€ä¸»è§’ã€‘ä¸å…¶ä»–äººçš„å…³ç³»ï¼ˆæœ‹å‹ã€æƒ…ä¾£ã€å®¶äººã€åŒäº‹ï¼‰
   - **äº’åŠ¨åŠ¨ä½œ**: ã€ä¸»è§’ã€‘ä¸å…¶ä»–äººçš„å…·ä½“äº’åŠ¨ï¼ˆå¦‚ï¼šã€ä¸»è§’ã€‘ä¸å¥³æ€§Aä¸¾æ¯åº†ç¥ï¼‰
   - **è¡¨æƒ…ç»†èŠ‚**: ã€ä¸»è§’ã€‘åŠå…¶ä»–äººç‰©çš„æƒ…ç»ªçŠ¶æ€ï¼ˆå¾®ç¬‘ã€ä¸¥è‚ƒã€æƒŠè®¶ï¼‰

3. **ç‰©å“ä¸æ¶ˆè´¹** (å®Œæ•´è¯†åˆ«):
   - **ä¸»è§’çš„ç‰©å“**: ã€ä¸»è§’ã€‘çš„æœè£…å“ç‰Œã€é£æ ¼ã€é…é¥°ã€ç”µå­è®¾å¤‡ç­‰
   - **æ¶ˆè´¹åœºæ™¯**: å…·ä½“é¤å…ç±»å‹ã€å•†åœºåç§°ã€æ™¯ç‚¹ç‰¹è‰²
   - **ç‰©å“ç»†èŠ‚**: ä¹¦ç±åç§°ã€å® ç‰©å“ç§ã€é£Ÿç‰©ç§ç±»ã€å“ç‰ŒLogo

4. **åŠ¨ä½œä¸æ´»åŠ¨** (å®Œæ•´è¯†åˆ«ï¼Œèšç„¦ä¸»è§’):
   - **ä¸»è§’çš„æ´»åŠ¨**: ã€ä¸»è§’ã€‘åœ¨åšä»€ä¹ˆï¼ˆèšé¤ã€æ—…è¡Œã€å·¥ä½œã€è¿åŠ¨ã€è´­ç‰©ã€ä¼‘é—²ï¼‰
   - **å…·ä½“åŠ¨ä½œ**: ã€ä¸»è§’ã€‘çš„åŠ¨ä½œç»†èŠ‚ï¼ˆè¡Œèµ°ã€åç€ã€ç«™ç«‹ã€è·‘æ­¥ã€æ‹ç…§ï¼‰
   - **äº’åŠ¨ç»†èŠ‚**: ã€ä¸»è§’ã€‘ä¸å…¶ä»–äººç‰©çš„äº’åŠ¨æ–¹å¼

5. **è§†è§‰é£æ ¼**:
   - å…‰çº¿ï¼ˆè‡ªç„¶å…‰ã€å®¤å†…ç¯å…‰ã€å¤œæ™¯ã€é€†å…‰ï¼‰
   - æ„å›¾ï¼ˆè‡ªæ‹ã€æŠ“æ‹ã€æ‘†æ‹ã€å…¨èº«ç…§ã€ç‰¹å†™ï¼‰
   - ä¿®å›¾ç¨‹åº¦ï¼ˆç²¾ä¿®ã€åŸå›¾ã€æ»¤é•œé£æ ¼ï¼‰

**é‡è¦æé†’**:
- â­ **é¦–è¦ä»»åŠ¡**: å‡†ç¡®è¯†åˆ«è°æ˜¯ç…§ç‰‡ä¸»äººï¼ˆä¸»è§’ï¼‰
- â­ **æ ‡è®°æ¸…æ™°**: åœ¨æ‰€æœ‰æè¿°ä¸­ï¼Œç”¨"ã€ä¸»è§’ã€‘"æ ‡è®°ä¸»äºº
- â­ **ä¸€è‡´æ€§**: ç¡®ä¿åŒä¸€æ‰¹æ¬¡çš„æè¿°ä¸­ï¼Œã€ä¸»è§’ã€‘æŒ‡ä»£åŒä¸€ä¸ªäºº
- è¯·ç¡®ä¿è¯†åˆ«æ¯ä¸€å¼ ç…§ç‰‡çš„ä¸»ä½“ã€ç¯å¢ƒå’ŒåŠ¨ä½œ
- ä¸è¦é—æ¼ä»»ä½•å¯è§çš„é‡è¦ç»†èŠ‚
- ä½ çš„æè¿°å°†æˆä¸ºåç»­äººæ ¼åˆ†æçš„åŸºç¡€ç´ æï¼Œä¸»è§’è¯†åˆ«å‡†ç¡®æ€§è‡³å…³é‡è¦"""

        return prompt

    async def analyze_batch(self, batch: Batch) -> Phase1Result:
        """åˆ†æå•ä¸ªæ‰¹æ¬¡"""
        # æ£€æŸ¥ç¼“å­˜
        cached = self._load_from_cache(batch.batch_id)
        if cached:
            return cached

        logger.info(f"ğŸ” æ­£åœ¨åˆ†ææ‰¹æ¬¡ {batch.batch_id} ({batch.image_count} å¼ ç…§ç‰‡)")

        # å‡†å¤‡å›¾ç‰‡ï¼ˆè·³è¿‡ä¸æ”¯æŒçš„æ ¼å¼ï¼‰
        images = []
        skipped = 0
        for photo in batch.photos[:50]:  # Flash æœ€å¤šå¤„ç†50å¼ 
            try:
                img = Image.open(photo.path)
                # æ£€æµ‹å¹¶è·³è¿‡ MPO æ ¼å¼ï¼ˆGemini API ä¸æ”¯æŒï¼‰
                if img.format == 'MPO':
                    logger.warning(f"âš ï¸ è·³è¿‡ MPO æ ¼å¼: {photo.path}")
                    img.close()
                    skipped += 1
                    continue
                images.append(img)
            except Exception as e:
                logger.warning(f"æ— æ³•è¯»å–å›¾ç‰‡ {photo.path}: {e}")

        if skipped > 0:
            logger.info(f"â„¹ï¸  æ‰¹æ¬¡ {batch.batch_id} è·³è¿‡ {skipped} ä¸ªä¸æ”¯æŒçš„æ–‡ä»¶")

        if len(images) == 0:
            logger.warning(f"âš ï¸  æ‰¹æ¬¡ {batch.batch_id} æ²¡æœ‰æœ‰æ•ˆå›¾ç‰‡")
            raw_output = "è¯¥æ‰¹æ¬¡æ²¡æœ‰æœ‰æ•ˆçš„å›¾ç‰‡å¯ä¾›åˆ†æ"
        else:
            # ç”Ÿæˆæç¤ºè¯
            prompt = self._create_prompt(batch)

            # è°ƒç”¨ Gemini Flash
            try:
                response = await asyncio.to_thread(
                    self.model.generate_content,
                    [prompt] + images
                )
                raw_output = response.text
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡ {batch.batch_id} åˆ†æå¤±è´¥: {e}")
                raw_output = f"åˆ†æå¤±è´¥: {str(e)}"

        # æ„å»ºç»“æœ
        result = Phase1Result(
            batch_id=batch.batch_id,
            processed_at=datetime.now().isoformat(),
            image_count=batch.image_count,
            time_range=(
                batch.time_range[0].isoformat(),
                batch.time_range[1].isoformat()
            ),
            raw_vlm_output=raw_output
        )

        # ä¿å­˜åˆ°ç¼“å­˜
        self._save_to_cache(result)

        return result

    async def analyze_all_batches(
        self,
        batches: List[Batch],
        max_concurrency: int = 3
    ) -> List[Phase1Result]:
        """å¹¶å‘åˆ†ææ‰€æœ‰æ‰¹æ¬¡"""
        logger.info(f"ğŸš€ å¼€å§‹ Phase 1 åˆ†æï¼Œå…± {len(batches)} ä¸ªæ‰¹æ¬¡")

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrency)

        async def process_with_semaphore(batch):
            async with semaphore:
                return await self.analyze_batch(batch)

        # å¹¶å‘å¤„ç†
        results = await asyncio.gather(
            *[process_with_semaphore(batch) for batch in batches]
        )

        logger.info(f"âœ… Phase 1 å®Œæˆï¼Œåˆ†æ {len(results)} ä¸ªæ‰¹æ¬¡")
        return results

# ==================== Phase 2: Reduce - æ·±åº¦äººæ ¼ç”»åƒ ====================

class Phase2Analyzer:
    """ä½¿ç”¨ Gemini Pro æ·±åº¦åˆ†æç”Ÿæˆå…­ç»´äººæ ¼ç”»åƒ"""

    def __init__(self):
        self.model = genai.GenerativeModel('models/gemini-2.5-pro')

    def _create_prompt(self, phase1_results: List[Phase1Result]) -> str:
        """ç”Ÿæˆ Phase 2 æç¤ºè¯ï¼ˆæ·±åº¦äººæ ¼åˆ†æï¼‰"""

        # æ±‡æ€»æ‰€æœ‰æ‰¹æ¬¡çš„æ—¶é—´çº¿
        timeline = "\n".join([
            f"- {r.batch_id}: {r.time_range[0][:7]} ~ {r.time_range[1][:7]} ({r.image_count}å¼ )"
            for r in sorted(phase1_results, key=lambda x: x.batch_id)
        ])

        # æ±‡æ€»æ‰€æœ‰å®¢è§‚æè¿°
        all_observations = "\n\n".join([
            f"## {r.batch_id}\n{r.raw_vlm_output}"
            for r in sorted(phase1_results, key=lambda x: x.batch_id)
        ])

        total_images = sum(r.image_count for r in phase1_results)

        prompt = f"""ä½ æ˜¯ä¸€ä½æ•°å­—äººç±»å­¦å®¶å’Œå¿ƒç†å­¦ä¸“å®¶ã€‚ç°åœ¨éœ€è¦ä½ åŸºäºç”¨æˆ·çš„å®Œæ•´ç›¸å†Œè®°å½•ï¼ˆè·¨åº¦{phase1_results[0].time_range[0][:7]}è‡³{phase1_results[-1].time_range[1][:7]}ï¼‰ï¼Œæ‰§è¡Œæ·±åº¦çš„ã€Šå…­ç»´äººæ ¼åˆ†æã€‹ã€‚

**æ•°æ®è§„æ¨¡**: å…±åˆ†æ {total_images} å¼ ç…§ç‰‡ï¼Œ{len(phase1_results)} ä¸ªæ—¶é—´æ®µ

**âš ï¸ æ ¸å¿ƒå‰æ**: æ‰€æœ‰è§‚å¯Ÿè®°å½•ä¸­çš„"ã€ä¸»è§’ã€‘"æ ‡è®°æŒ‡ä»£ç…§ç‰‡ä¸»äººï¼ˆç”¨æˆ·æœ¬äººï¼‰

**åˆ†æå¯¹è±¡è¯´æ˜**:
- ä½ çš„åˆ†æå¯¹è±¡æ˜¯"ã€ä¸»è§’ã€‘"ï¼ˆç…§ç‰‡ä¸»äººï¼‰ï¼Œä¸æ˜¯å…¶ä»–äººç‰©
- æ‰€æœ‰äººæ ¼æ¨æ–­ã€å¿ƒç†åˆ†æã€è¡Œä¸ºæ¨¡å¼éƒ½åº”åŸºäº"ã€ä¸»è§’ã€‘"çš„è¡¨ç°
- å…¶ä»–äººç‰©ï¼ˆå¥³æ€§Aã€ç”·æ€§Bç­‰ï¼‰ä»…ä½œä¸º"ã€ä¸»è§’ã€‘"ç¤¾äº¤å…³ç³»çš„å‚è€ƒ

**æ—¶é—´çº¿**:
{timeline}

**å®Œæ•´è§‚å¯Ÿè®°å½•**ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰:
{all_observations}

---

**ä»»åŠ¡**: è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON Schema è¾“å‡ºåˆ†æç»“æœï¼š

```json
{{
  "meta": {{
    "scan_summary": "å·²åˆ†æ X å¼ å›¾ç‰‡ï¼Œè·¨åº¦ YYYY.MM-YYYY.MM",
    "timeline_chapters": ["AI åˆ’åˆ†çš„äººç”Ÿé˜¶æ®µï¼Œå¦‚ï¼š'2023ä¸ŠåŠå¹´: è¿·èŒ«æ¢ç´¢æœŸ', '2023ä¸‹åŠå¹´: æƒ…æ„Ÿç¨³å®šæœŸ'"]
  }},
  "L1_Spatio_Temporal": {{
    "life_radius": "åŸºäºGPSåˆ†æã€ä¸»è§’ã€‘çš„æ´»åŠ¨èŒƒå›´ï¼Œå¦‚ï¼šä¸¤ç‚¹ä¸€çº¿(å®¶-å…¬å¸) vs åŸå¸‚æ¼«æ¸¸è€… vs å…¨çƒæ¸¸ç‰§",
    "biological_clock": "åŸºäºæ‹æ‘„æ—¶é—´æˆ³åˆ†æã€ä¸»è§’ã€‘çš„ä½œæ¯ï¼Œå¦‚ï¼šæ—©èµ·å‹(7-9ç‚¹æ´»è·ƒ) vs æ·±å¤œæ´»è·ƒå‹(22-2ç‚¹) vs å‘¨æœ«ç‹‚æ¬¢è€…"
  }},
  "L3_Social_Graph": {{
    "core_circle": [
      {{"name_id": "å¥³æ€§A", "relation": "æ¨æµ‹ä¸ºã€ä¸»è§’ã€‘çš„ä¼´ä¾£", "frequency": "é«˜é¢‘å‡ºç°", "status": "ç¨³å®š"}},
      {{"name_id": "ç”·æ€§B", "relation": "ã€ä¸»è§’ã€‘çš„å·¥ä½œä¼™ä¼´", "frequency": "ä»…åœ¨å·¥ä½œåœºæ™¯", "status": "Q4åæ¶ˆå¤±"}}
    ],
    "relationship_dynamics": ["ã€ä¸»è§’ã€‘ç¤¾äº¤å…³ç³»çš„å…³é”®å˜åŒ–ï¼Œå¦‚ï¼š'å¥³æ€§Aåœ¨Q2é¦–æ¬¡å‡ºç° - ç–‘ä¼¼ã€ä¸»è§’ã€‘å¼€å§‹æ‹æƒ…'", 'ç”·æ€§Båœ¨Q4æ¶ˆå¤± - å¯èƒ½ç¦»èŒæˆ–ä¸ã€ä¸»è§’ã€‘å…³ç³»ç–è¿œ'"]
  }},
  "L4_Behavior_Trends": {{
    "social_mask": "åˆ†æã€ä¸»è§’ã€‘ç²¾ä¿®å›¾ä¸éšæ‰‹æ‹çš„æ¯”ä¾‹ï¼Œåˆ¤æ–­å¶åƒåŒ…è¢±ï¼Œå¦‚ï¼šé«˜ç²¾è‡´åº¦(æœ‹å‹åœˆäººè®¾) vs çœŸå®è®°å½•å‹",
    "consumption_shift": "åˆ†æã€ä¸»è§’ã€‘çš„ç‰©å“/åœºæ™¯å˜åŒ–ï¼Œå¦‚ï¼š'ä»å¿«æ—¶å°šè½¬å‘éœ²è¥è£…å¤‡', 'ä»å’–å•¡å…è½¬å‘å¥èº«æˆ¿'"
  }},
  "L5_Psychology": {{
    "personality_type": "åŸºäºã€ä¸»è§’ã€‘çš„è¡Œä¸ºæ¨¡å¼æ¨æµ‹MBTIæˆ–å¤§äº”äººæ ¼ï¼Œå¦‚ï¼šå¤–å‘è¡¨è¾¾å‹(ENFP) vs å†…çœè§‚å¯Ÿè€…(INFP)",
    "emotional_curve": "æè¿°ã€ä¸»è§’ã€‘çš„æƒ…ç»ªæ³¢åŠ¨å‘¨æœŸï¼Œå¦‚ï¼š'Q1æƒ…ç»ªä½è½(ç‹¬å¤„ç…§ç‰‡å¤š) â†’ Q2å¼€å§‹ç¤¾äº¤æ´»è·ƒ â†’ Q3æƒ…ç»ªé«˜æ¶¨'"
  }},
  "L6_Hooks": {{
    "story_trigger": "ç”Ÿæˆä¸€å¥ç›´å‡»ã€ä¸»è§’ã€‘å¿ƒçµçš„æé—®ï¼Œå¦‚ï¼š'ä¸ºä»€ä¹ˆä½ åœ¨2023å¹´5æœˆçªç„¶å¼€å§‹é¢‘ç¹å»å¥èº«æˆ¿ï¼Ÿ'"
  }}
}}
```

**åˆ†æåŸåˆ™**:
1. **åŸºäºè¯æ®**: æ¯ä¸ªå…³äºã€ä¸»è§’ã€‘çš„æ¨æ–­éƒ½è¦æœ‰ç…§ç‰‡è§‚å¯Ÿä½œä¸ºæ”¯æ’‘
2. **å…³æ³¨å˜åŒ–**: é‡ç‚¹åˆ†æã€ä¸»è§’ã€‘åœ¨æ—¶é—´çº¿ä¸Šçš„è½¬æŠ˜ç‚¹
3. **æ·±åº¦æ´å¯Ÿ**: ä¸è¦åœç•™åœ¨è¡¨é¢æè¿°ï¼Œè¦æŒ–æ˜ã€ä¸»è§’ã€‘è¡Œä¸ºèƒŒåçš„å¿ƒç†åŠ¨æœº
4. **ä¸»è§’èšç„¦**: æ‰€æœ‰äººæ ¼åˆ†æéƒ½å›´ç»•ã€ä¸»è§’ã€‘å±•å¼€ï¼Œå…¶ä»–äººç‰©ä»…ä½œä¸ºã€ä¸»è§’ã€‘ç¤¾äº¤å…³ç³»çš„èƒŒæ™¯
5. **äººæ–‡å…³æ€€**: ç”¨æ¸©æš–è€Œä¸“ä¸šçš„è¯­è°ƒï¼Œå±•ç°å¯¹ã€ä¸»è§’ã€‘è¿™ä¸ª"æ•°å­—ç”Ÿå‘½"çš„ç†è§£

è¯·ç›´æ¥è¾“å‡º JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""

        return prompt

    def analyze(self, phase1_results: List[Phase1Result]) -> Dict[str, Any]:
        """æ‰§è¡Œ Phase 2 æ·±åº¦åˆ†æ"""
        logger.info("ğŸ§  å¼€å§‹ Phase 2 æ·±åº¦äººæ ¼åˆ†æ...")

        prompt = self._create_prompt(phase1_results)

        try:
            response = self.model.generate_content(prompt)
            raw_json = response.text.strip()

            # æå– JSONï¼ˆå»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°ï¼‰
            json_match = re.search(r'```json\s*(.*?)\s*```', raw_json, re.DOTALL)
            if json_match:
                raw_json = json_match.group(1)
            else:
                # å°è¯•æå–ç¬¬ä¸€ä¸ª { } åŒ…è£¹çš„å†…å®¹
                json_match = re.search(r'\{.*\}', raw_json, re.DOTALL)
                if json_match:
                    raw_json = json_match.group(0)

            result = json.loads(raw_json)

            logger.info("âœ… Phase 2 åˆ†æå®Œæˆ")
            return result

        except Exception as e:
            logger.error(f"Phase 2 åˆ†æå¤±è´¥: {e}")
            return {
                "error": str(e),
                "raw_output": response.text if 'response' in locals() else None
            }

# ==================== ä¸»æ§åˆ¶å™¨ ====================

class MemoryProfiler:
    """AI è®°å¿†åˆ†æå™¨ä¸»æ§åˆ¶å™¨"""

    def __init__(self, photo_dir: str):
        self.photo_dir = Path(photo_dir)
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)

    def scan_photos(self) -> List[PhotoMetadata]:
        """æ‰«ææ‰€æœ‰ç…§ç‰‡"""
        logger.info(f"ğŸ“ æ‰«æç…§ç‰‡ç›®å½•: {self.photo_dir}")

        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼ï¼ˆæ’é™¤ .mpo ç­‰ Gemini ä¸æ”¯æŒçš„æ ¼å¼ï¼‰
        extensions = {'.jpg', '.jpeg', '.png', '.heic', '.webp'}

        # æ’é™¤çš„æ ¼å¼ï¼ˆGemini API ä¸æ”¯æŒï¼‰
        excluded_extensions = {'.mpo'}

        photos = []
        for ext in extensions:
            for photo_path in self.photo_dir.glob(f"**/*{ext}"):
                photos.append(str(photo_path))

        # æ’é™¤ä¸æ”¯æŒçš„æ ¼å¼
        filtered_photos = []
        for photo in photos:
            ext = Path(photo).suffix.lower()
            if ext not in excluded_extensions:
                filtered_photos.append(photo)
            else:
                logger.warning(f"âš ï¸ è·³è¿‡ä¸æ”¯æŒçš„æ ¼å¼: {photo}")

        logger.info(f"ğŸ“¸ å‘ç° {len(filtered_photos)} å¼ ç…§ç‰‡ï¼ˆå·²è¿‡æ»¤ä¸æ”¯æŒçš„æ ¼å¼ï¼‰")

        # å¹¶å‘æå– EXIF
        with ThreadPoolExecutor(max_workers=10) as executor:
            photos_metadata = list(executor.map(EXIFExtractor.process_photo, filtered_photos))

        # æŒ‰æ—¶é—´æ’åº
        photos_metadata.sort(key=lambda x: x.datetime)

        logger.info(f"âœ… EXIF æå–å®Œæˆï¼Œæ—¶é—´è·¨åº¦: "
                   f"{photos_metadata[0].datetime.strftime('%Y-%m-%d')} ~ "
                   f"{photos_metadata[-1].datetime.strftime('%Y-%m-%d')}")

        return photos_metadata

    def run(self, phase2_only: bool = False) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´åˆ†ææµç¨‹

        Args:
            phase2_only: å¦‚æœä¸º Trueï¼Œè·³è¿‡ Phase 1ï¼Œç›´æ¥ä»ç¼“å­˜åŠ è½½å¹¶è¿è¡Œ Phase 2
        """
        # Phase 0: æ‰«æç…§ç‰‡
        photos = self.scan_photos()

        # Phase 0: åˆ›å»ºæ‰¹æ¬¡
        batch_processor = BatchProcessor()
        batches = batch_processor.create_batches(photos)
        logger.info(f"ğŸ“¦ ç”Ÿæˆ {len(batches)} ä¸ªæ‰¹æ¬¡")

        # Phase 1: Map - å®¢è§‚äº‹å®æå–
        if not phase2_only:
            phase1_analyzer = Phase1Analyzer()
            phase1_results = asyncio.run(phase1_analyzer.analyze_all_batches(batches))

            # ä¿å­˜ Phase 1 ç»“æœæ±‡æ€»
            summary_path = self.cache_dir / "phase1_summary.json"
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump([asdict(r) for r in phase1_results], f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ Phase 1 æ±‡æ€»å·²ä¿å­˜: {summary_path}")
        else:
            # ä»ç¼“å­˜åŠ è½½
            logger.info("ğŸ”„ Phase 2 æ¨¡å¼ï¼šä»ç¼“å­˜åŠ è½½ Phase 1 ç»“æœ...")
            summary_path = self.cache_dir / "phase1_summary.json"
            if not summary_path.exists():
                raise FileNotFoundError("æœªæ‰¾åˆ° Phase 1 ç¼“å­˜æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œå®Œæ•´æµç¨‹")

            with open(summary_path, 'r', encoding='utf-8') as f:
                phase1_data = json.load(f)
                phase1_results = [Phase1Result(**d) for d in phase1_data]
            logger.info(f"âœ… åŠ è½½ {len(phase1_results)} ä¸ªæ‰¹æ¬¡çš„ç¼“å­˜æ•°æ®")

        # Phase 2: Reduce - æ·±åº¦äººæ ¼ç”»åƒ
        phase2_analyzer = Phase2Analyzer()
        final_profile = phase2_analyzer.analyze(phase1_results)

        # ä¿å­˜æœ€ç»ˆç»“æœ
        output_path = f"memory_profile_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_profile, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ‰ åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜: {output_path}")

        return final_profile

# ==================== CLI å…¥å£ ====================

def main():
    import argparse

    parser = argparse.ArgumentParser(description='AI è®°å¿†åˆ†æå™¨')
    parser.add_argument('photo_dir', help='ç…§ç‰‡ç›®å½•è·¯å¾„')
    parser.add_argument('--phase2-only', action='store_true',
                       help='ä»…è¿è¡Œ Phase 2ï¼ˆä»ç¼“å­˜åŠ è½½ Phase 1 ç»“æœï¼‰')

    args = parser.parse_args()

    profiler = MemoryProfiler(args.photo_dir)
    result = profiler.run(phase2_only=args.phase2_only)

    # æ‰“å°æ‘˜è¦
    print("\n" + "="*70)
    print("ğŸ“Š åˆ†ææ‘˜è¦")
    print("="*70)
    if "error" not in result:
        print(f"ğŸ¯ {result['meta']['scan_summary']}")
        print(f"\nğŸ“– äººç”Ÿé˜¶æ®µ:")
        for chapter in result['meta']['timeline_chapters']:
            print(f"  - {chapter}")
        print(f"\nğŸ­ äººæ ¼ç±»å‹: {result['L5_Psychology']['personality_type']}")
        print(f"ğŸ’¬ çµé­‚æé—®: {result['L6_Hooks']['story_trigger']}")
    else:
        print(f"âŒ åˆ†æå¤±è´¥: {result['error']}")
    print("="*70)

if __name__ == "__main__":
    main()
